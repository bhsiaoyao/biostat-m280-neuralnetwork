---
title: "Biostat M280 Homework 4"
subtitle: Due Mar 22 @ 11:59PM
author: Li Zhang lizhang1122@ucla.edu
output: html_document
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For this homework, you can work as a team of size $\le 5$. You can create a new private GitHub repository for collaboration (need to add @Hua-Zhou and @juhkim111 as collaborators) or re-use the current repository of a team representative. For each question, your report should have a clear description of role of each team member, and Git log should reflect individual contribution to the project.

## Q1 Learn by doing

I found the [TensorFlow for R Blog](https://blogs.rstudio.com/tensorflow/) series at RStudio quite illuminating. Choose one blog that interests you and do following.  

1. Reproduce the results in the blog.  

    **The blog is "Classifying physical activity from smartphone data" with html link: https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection/

    **Then reproduce the results in the blog as follows:

    **Introduction
    In this post we’ll describe how to use smartphone accelerometer and gyroscope data to predict the physical activities of the individuals carrying the phones. The data used in this post comes from the Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set distributed by the University of California, Irvine. Thirty individuals were tasked with performing various basic activities with an attached smartphone recording movement using an accelerometer and gyroscope.
    Before we begin, let’s load the various libraries that we’ll use in the analysis:

    ```{r}
    library(keras)     # Neural Networks
    library(tidyverse) # Data cleaning / Visualization
    library(knitr)     # Table printing
    library(rmarkdown) # Misc. output utilities 
    install.packages("ggridges")
    library(ggridges)  # Visualization
    library(tensorflow)
    install_tensorflow()
    ```

    **Activities dataset
    The data used in this post come from the Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set distributed by the University of California, Irvine.
    
    ```{r, eval = FALSE}
    dir.create("data")
    download.file(
      url = "http://archive.ics.uci.edu/ml/machine-learning-databases/00341/HAPT%20Data%20Set.zip", 
      destfile = "data/HAPT.zip"
      )
    unzip("data/HAPT.zip", exdir = "data/HAPT")
    ```
    
    **Activity labels
The data has integer encodings for the activities which, while not important to the model itself, are helpful for use to see. Let’s load them first.

    ```{r}
    activityLabels <- read.table("data/HAPT/activity_labels.txt", 
                                 col.names = c("number", "label")) 
    activityLabels %>% kable(align = c("c", "l"))
    ```

Next, we load in the labels key for the RawData. This file is a list of all of the observations, or individual activity recordings, contained in the data set. The key for the columns is taken from the data README.txt.

    Column 1: experiment number ID, 
    Column 2: user number ID, 
    Column 3: activity number ID, 
    Column 4: Label start point,
    Column 5: Label end point.

Let’s take a look at the first 50 rows:

    ```{r}
    labels <- read.table("data/HAPT/RawData/labels.txt", col.names = c("experiment", "userId", "activity", "startPos", "endPos"))
    labels %>% 
      head(50) %>% 
      paged_table()
    ```

    **File names
Next, let’s look at the actual files of the user data provided to us in RawData

    ```{r}
    dataFiles <- list.files("data/HAPT/RawData")
    dataFiles %>% head()
    ```

There is a three-part file naming scheme. The first part is the type of data the file contains: either acc for accelerometer or gyro for gyroscope. Next is the experiment number, and last is the user Id for the recording. Let’s load these into a dataframe for ease of use later.

    ```{r}
    fileInfo <- data_frame(filePath = dataFiles) %>%
      filter(filePath != "labels.txt") %>% 
      separate(filePath, sep = '_', 
           into = c("type", "experiment", "userId"), 
           remove = FALSE) %>% 
      mutate(
        experiment = str_remove(experiment, "exp"),
        userId = str_remove_all(userId, "user|\\.txt")) %>% 
      spread(type, filePath)
    
    fileInfo %>% head() %>% kable()
    ```

    **Reading and gathering data
Before we can do anything with the data provided we need to get it into a model-friendly format. This means we want to have a list of observations, their class (or activity label), and the data corresponding to the recording.

To obtain this we will scan through each of the recording files present in dataFiles, look up what observations are contained in the recording, extract those recordings and return everything to an easy to model with dataframe.

    ```{r}
    readInData <- function(experiment, userId){
      genFilePath = function(type) {
        paste0("data/HAPT/RawData/", type, "_exp",experiment, 
               "_user", userId, ".txt")
        }  
      bind_cols(
        read.table(genFilePath("acc"), col.names = c("a_x", "a_y", "a_z")),
        read.table(genFilePath("gyro"), col.names = c("g_x", "g_y", "g_z"))
      )
    }
    
    # Function to read a given file and get the observations contained along
    # with their classes.
    
    loadFileData <- function(curExperiment, curUserId) {
      # load sensor data from file into dataframe
      allData <- readInData(curExperiment, curUserId)
      
      extractObservation <- function(startPos, endPos){
        allData[startPos:endPos,]
        }  
      # get observation locations in this file from labels dataframe
      dataLabels <- labels %>%
        filter(userId == as.integer(curUserId), 
           experiment == as.integer(curExperiment))
      
      # extract observations as dataframes and save as a column in dataframe.
      dataLabels %>%
        mutate(data = map2(startPos, endPos, extractObservation)) %>%
        select(-startPos, -endPos)
      }
    # scan through all experiment and userId combos and gather data into a dataframe. 
    allObservations <- map2_df(fileInfo$experiment, fileInfo$userId, loadFileData) %>%
      right_join(activityLabels, by = c("activity" = "number")) %>% 
      rename(activityName = label)
    # cache work. 
    write_rds(allObservations, "allObservations.rds")
    allObservations %>% dim()
    ```


    **Exploring the data
Now that we have all the data loaded along with the experiment, userId, and activity labels, we can explore the data set.

    **Length of recordings
Let’s first look at the length of the recordings by activity.

    ```{r}
    allObservations %>% 
      mutate(recording_length = map_int(data,nrow)) %>% 
      ggplot(aes(x = recording_length, y = activityName)) +
      geom_density_ridges(alpha = 0.8)
    ```

The fact there is such a difference in length of recording between the different activity types requires us to be a bit careful with how we proceed. If we train the model on every class at once we are going to have to pad all the observations to the length of the longest, which would leave a large majority of the observations with a huge proportion of their data being just padding-zeros. Because of this, we will fit our model to just the largest ‘group’ of observations length activities, these include STAND_TO_SIT, STAND_TO_LIE, SIT_TO_STAND, SIT_TO_LIE, LIE_TO_STAND, and LIE_TO_SIT.


    **Filtering activities
Based on our work from above, let’s subset the data to just be of the activities of interest.

    ```{r}
    desiredActivities <- c(
  "STAND_TO_SIT", "SIT_TO_STAND", "SIT_TO_LIE", 
  "LIE_TO_SIT", "STAND_TO_LIE", "LIE_TO_STAND")
    
    filteredObservations <- allObservations %>% 
      filter(activityName %in% desiredActivities) %>% 
      mutate(observationId = 1:n())
    
    filteredObservations %>% paged_table()
    ```

So after our aggressive pruning of the data we will have a respectable amount of data left upon which our model can learn.


    **Training/testing split
Before we go any further into exploring the data for our model, in an attempt to be as fair as possible with our performance measures, we need to split the data into a train and test set. Since each user performed all activities just once (with the exception of one who only did 10 of the 12 activities) by splitting on userId we will ensure that our model sees new people exclusively when we test it.

    ```{r}
    # get all users
    userIds <- allObservations$userId %>% unique()

    # randomly choose 24 (80% of 30 individuals) for training
    set.seed(42) # seed for reproducibility
    trainIds <- sample(userIds, size = 24)

    # set the rest of the users to the testing set
    testIds <- setdiff(userIds,trainIds)

    # filter data. 
    trainData <- filteredObservations %>% 
      filter(userId %in% trainIds)

    testData <- filteredObservations %>% 
      filter(userId %in% testIds)
    ```


    **Visualizing activities
Now that we have trimmed our data by removing activities and splitting off a test set, we can actually visualize the data for each class to see if there’s any immediately discernible shape that our model may be able to pick up on.

First let’s unpack our data from its dataframe of one-row-per-observation to a tidy version of all the observations.

    ```{r}
    unpackedObs <- 1:nrow(trainData) %>% 
      map_df(function(rowNum){
        dataRow <- trainData[rowNum, ]
        dataRow$data[[1]] %>% 
          mutate(
            activityName = dataRow$activityName, 
            observationId = dataRow$observationId,
            time = 1:n() )
        }) %>% 
      gather(reading, value, -time, -activityName, -observationId) %>% 
      separate(reading, into = c("type", "direction"), sep = "_") %>% 
      mutate(type = ifelse(type == "a", "acceleration", "gyro"))
    ```

Now we have an unpacked set of our observations, let’s visualize them!

    ```{r}
    unpackedObs %>% 
      ggplot(aes(x = time, y = value, color = direction)) +
      geom_line(alpha = 0.2) +
      geom_smooth(se = FALSE, alpha = 0.7, size = 0.5) +
      facet_grid(type ~ activityName, scales = "free_y") +
      theme_minimal() +
      theme( axis.text.x = element_blank() )
    ```

So at least in the accelerometer data patterns definitely emerge. One would imagine that the model may have trouble with the differences between LIE_TO_SIT and LIE_TO_STAND as they have a similar profile on average. The same goes for SIT_TO_STAND and STAND_TO_SIT.


    **Preprocessing
Before we can train the neural network, we need to take a couple of steps to preprocess the data.

    **Padding observations
First we will decide what length to pad (and truncate) our sequences to by finding what the 98th percentile length is. By not using the very longest observation length this will help us avoid extra-long outlier recordings messing up the padding.

    ```{r}
    padSize <- trainData$data %>% 
      map_int(nrow) %>% 
      quantile(p = 0.98) %>% 
      ceiling()
    padSize
    ```

Now we simply need to convert our list of observations to matrices, then use the super handy pad_sequences() function in Keras to pad all observations and turn them into a 3D tensor for us.

    ```{r}
    convertToTensor <- . %>% 
      map(as.matrix) %>% 
      pad_sequences(maxlen = padSize)
    
    trainObs <- trainData$data %>% convertToTensor()
    testObs <- testData$data %>% convertToTensor()
    
    dim(trainObs)
    ```

Wonderful, we now have our data in a nice neural-network-friendly format of a 3D tensor with dimensions


    **One-hot encoding
There’s one last thing we need to do before we can train our model, and that is turn our observation classes from integers into one-hot, or dummy encoded, vectors. Luckily, again Keras has supplied us with a very helpful function to do just this.

    ```{r}
    oneHotClasses <- . %>% 
      {. - 7} %>%        # bring integers down to 0-6 from 7-12
      to_categorical() # One-hot encode
    
    trainY <- trainData$activity %>% oneHotClasses()
    testY <- testData$activity %>% oneHotClasses()
    
    ```


    **Modeling
    **Architecture
Since we have temporally dense time-series data we will make use of 1D convolutional layers. With temporally-dense data, an RNN has to learn very long dependencies in order to pick up on patterns, CNNs can simply stack a few convolutional layers to build pattern representations of substantial length. Since we are also simply looking for a single classification of activity for each observation, we can just use pooling to ‘summarize’ the CNNs view of the data into a dense layer.
In addition to stacking two layer_conv_1d() layers, we will use batch norm and dropout to regularize the network.

    ```{r}
    input_shape <- dim(trainObs)[-1]
    num_classes <- dim(trainY)[2]
    filters <- 24     # number of convolutional filters to learn
    kernel_size <- 8  # how many time-steps each conv layer sees.
    dense_size <- 48  # size of our penultimate dense layer. 
   
    # Initialize model
    model <- keras_model_sequential()
    model %>%
      layer_conv_1d(
        filters = filters,
        kernel_size = kernel_size, 
        input_shape = input_shape,
        padding = "valid", 
        activation = "relu") %>%
      layer_batch_normalization() %>%
      layer_spatial_dropout_1d(0.15) %>% 
      layer_conv_1d(
        filters = filters/2,
        kernel_size = kernel_size,
        activation = "relu",) %>%
      # Apply average pooling:
      layer_global_average_pooling_1d() %>% 
      layer_batch_normalization() %>%
      layer_dropout(0.2) %>% 
      layer_dense(
        dense_size,
        activation = "relu"
        ) %>% 
      layer_batch_normalization() %>%
      layer_dropout(0.25) %>%
      layer_dense(
        num_classes, 
        activation = "softmax",
        name = "dense_output"
        ) 
    
    summary(model)
    ```


    **Training
Now we can train the model using our test and training data. Note that we use callback_model_checkpoint() to ensure that we save only the best variation of the model (desirable since at some point in training the model may begin to overfit or otherwise stop improving).


    ```{r}
    # Compile model
    model %>% compile(
      loss = "categorical_crossentropy",
      optimizer = "rmsprop",
      metrics = "accuracy"
      )
    
    trainHistory <- model %>%
      fit(
        x = trainObs, y = trainY,
        epochs = 350,
        validation_data = list(testObs, testY),
        callbacks = list(
          callback_model_checkpoint("best_model.h5",
                                    save_best_only = TRUE)
          )
        )
    ```


The model is learning something! We get a respectable 94.4% accuracy on the validation data, not bad with six possible classes to choose from. Let’s look into the validation performance a little deeper to see where the model is messing up.



    **Evaluation
Now that we have a trained model let’s investigate the errors that it made on our testing data. We can load the best model from training based upon validation accuracy and then look at each observation, what the model predicted, how high a probability it assigned, and the true activity label.

    ```{r}
    # dataframe to get labels onto one-hot encoded prediction columns
    oneHotToLabel <- activityLabels %>% 
      mutate(number = number - 7) %>% 
      filter(number >= 0) %>% 
      mutate(class = paste0("V",number + 1)) %>% 
      select(-number)
    
    # Load our best model checkpoint
    bestModel <- load_model_hdf5("best_model.h5")
    tidyPredictionProbs <- bestModel %>% 
      predict(testObs) %>% 
      as_data_frame() %>% 
      mutate(obs = 1:n()) %>% 
      gather(class, prob, -obs) %>% 
      right_join(oneHotToLabel, by = "class")
    
    predictionPerformance <- tidyPredictionProbs %>% 
      group_by(obs) %>% 
      summarise(
        highestProb = max(prob),
        predicted = label[prob == highestProb]) %>% 
      mutate(
        truth = testData$activityName,
        correct = truth == predicted
        ) 
    
    predictionPerformance %>% paged_table()
    ```

First, let’s look at how ‘confident’ the model was by if the prediction was correct or not.

    ```{r}
    predictionPerformance %>% 
      mutate(result = ifelse(correct, 'Correct', 'Incorrect')) %>% 
      ggplot(aes(highestProb)) +
      geom_histogram(binwidth = 0.01) +
      geom_rug(alpha = 0.5) +
      facet_grid(result~.) +
      ggtitle("Probabilities associated with prediction by correctness")
    ```


Reassuringly it seems the model was, on average, less confident about its classifications for the incorrect results than the correct ones. (Although, the sample size is too small to say anything definitively.)

Let’s see what activities the model had the hardest time with using a confusion matrix.

    ```{r}
    predictionPerformance %>% 
      group_by(truth, predicted) %>% 
      summarise(count = n()) %>% 
      mutate(good = truth == predicted) %>% 
      ggplot(aes(x = truth,  y = predicted)) +
      geom_point(aes(size = count, color = good)) +
      geom_text(aes(label = count), 
                hjust = 0, vjust = 0, nudge_x = 0.1, nudge_y = 0.1) + 
      guides(color = FALSE, size = FALSE) +
      theme_minimal()
    ```


We see that, as the preliminary visualization suggested, the model had a bit of trouble with distinguishing between LIE_TO_SIT and LIE_TO_STAND classes, along with the SIT_TO_LIE and STAND_TO_LIE, which also have similar visual profiles.






















2. Make your own tweaks. For example, try different tuning parameter values and report what you found, or try a new data set, or apply the method to a new application.

## Q2 Deep learning on smart phone

Professor May Wang in Department of Community Health Sciences (CHS) studies obesity in children and intervention strategies to prevent obesity. She asked me whether it is possible to develop an app such that a user takes a photo of a meal and the app will recognize and record the type of food (pizza, mac and cheese, burger, ...). 

Your job: produce a prototype app for iPhone or Android smart phone. 

Resources:  
1. There are plenty of tutorials and YouTube clips on making apps for iPhone or Android.  
2. Google's [Cloud Vision API](https://cloud.google.com/vision/) may supply an easy cloud solution.  
3. [TensorFlow Lite](https://www.tensorflow.org/lite) may provide an easy mobile solution.  
